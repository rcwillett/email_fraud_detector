{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f76708",
   "metadata": {},
   "source": [
    "# Email Fraud Detector: BERT Model Build\n",
    "\n",
    "#### Ross Willett"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5850c",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Email-Fraud-Detector:-BERT-Model-Build\" data-toc-modified-id=\"Email-Fraud-Detector:-BERT-Model-Build-1\">Email Fraud Detector: BERT Model Build</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Ross-Willett\" data-toc-modified-id=\"Ross-Willett-1.0.0.1\">Ross Willett</a></span></li></ul></li></ul></li><li><span><a href=\"#File-Introduction\" data-toc-modified-id=\"File-Introduction-1.1\">File Introduction</a></span></li><li><span><a href=\"#Preparing-the-Model\" data-toc-modified-id=\"Preparing-the-Model-1.2\">Preparing the Model</a></span></li><li><span><a href=\"#Building-the-Model\" data-toc-modified-id=\"Building-the-Model-1.3\">Building the Model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33790e",
   "metadata": {},
   "source": [
    "## File Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adedfe1",
   "metadata": {},
   "source": [
    "In this file, a BERT model loaded from [TensorflowHub](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4) with additional layers added on top of it for email classification will be trained. A BERT model will be used as the base for a transfer learning model for several reasons. One reason a BERT model will be used is due to the fact it uses word embeddings which will place words with similar meanings in a relatively close vector space. This means that the model will not rely on specific words in order to positively identify a fraudulent email, thus making the model more generalizable. In addition to this, the BERT model takes into account the context of words in relation to other words in the input. (Up to 512 words) This will allow the model to better numerically express the context of the text content which should allow for a better ability to classify an email as fraudulent or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a4e613",
   "metadata": {},
   "source": [
    "## Preparing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860d400d",
   "metadata": {},
   "source": [
    "Before the model can be built and trained, the data needs to be prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "768889ec-9115-4425-b597-48b937c1b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Model selection libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model Evaluation Libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Import Tensor Flow and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Import Tensor Flow Hub and Tensor Flow Text (Required libraries for the pre-trained BERT Model)\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "\n",
    "# Import library for saving files\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b43b1f2-e02c-4997-b455-4d25c47bbfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import warnings and supress them\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9059fde-f475-410c-8ddf-ccca9e618482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Pandas to show all columns / rows\n",
    "pd.options.display.max_columns = 2000\n",
    "pd.options.display.max_rows = 2000\n",
    "# Set column max width larger\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e71215b-9235-44d3-9886-19b84339bc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load X remainder\n",
    "X_remainder = pd.read_csv('./data/X_remainder.csv')\n",
    "# Load X test\n",
    "X_test = pd.read_csv('./data/X_test.csv')\n",
    "# Load y remainder\n",
    "y_remainder = pd.read_csv('./data/y_remainder.csv')\n",
    "# Load y test\n",
    "y_test = pd.read_csv('./data/y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac732b",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68009960",
   "metadata": {},
   "source": [
    "Now that the data has been appropriately separated, the model can be built and trained. First the BERT encoder and pre-trained BERT model to be built on needs to be loaded from Tensor Flow Hub. The BERT model that will be used is a more compact version of the original model so as to allow for faster training and testing. This model consists of 12 hidden layers (i.e. Transformer blocks), a layer node size of 768, and 12 attention heads. For the purposes of this project, the BERT enoding layers will be frozen such that only the layers added to the output will be trained. This will be done since the BERT model has already been trained upon a large data set of words and will output appropriate relationships between these words, and therefore should require no further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b085d065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT encoder from Tensor Flow Hub\n",
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "# Load the BERT model from Tensor Flow Hub\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a22a97eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text_input \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mstring)\n\u001b[0;32m----> 2\u001b[0m encoder_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m(text_input)\n\u001b[1;32m      3\u001b[0m outputs \u001b[38;5;241m=\u001b[39m encoder(encoder_inputs)\n\u001b[1;32m      5\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooled_output\u001b[39m\u001b[38;5;124m\"\u001b[39m]      \u001b[38;5;66;03m# [batch_size, 768].\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "# Instantiate the input layer of the model\n",
    "text_input = layers.Input(shape=(), dtype=tf.string)\n",
    "# Pass the input layer to the BERT tokenizing layer\n",
    "encoder_inputs = preprocessor(text_input)\n",
    "# Pass the tokenized input to the BERT encoder\n",
    "outputs = encoder(encoder_inputs)\n",
    "# Get the pooled output of the BERT encoder\n",
    "pooled_output = outputs[\"pooled_output\"]\n",
    "# Pass the pooled output of the BERT encoder to a 128 node relu layer\n",
    "relu_layer = layers.Dense(128, activation='relu')(pooled_output)\n",
    "# Pass the relu layer to a 1 node sigmoid layer for classification\n",
    "output = layers.Dense(1, activation='sigmoid')(relu_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced736a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39mtext_input, outputs\u001b[38;5;241m=\u001b[39m\u001b[43moutput\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "bert_model = tf.keras.Model(inputs=text_input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d74790b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       {'input_mask': (Non  0           ['input_1[0][0]']                \n",
      "                                e, 128),                                                          \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " keras_layer_1 (KerasLayer)     {'encoder_outputs':  109482241   ['keras_layer[0][0]',            \n",
      "                                 [(None, 128, 768),               'keras_layer[0][1]',            \n",
      "                                 (None, 128, 768),                'keras_layer[0][2]']            \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)],                                               \n",
      "                                 'default': (None,                                                \n",
      "                                768),                                                             \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 768)}                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          98432       ['keras_layer_1[0][13]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,580,802\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Examine the layers of the model\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93380af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the BERT model using the adam optimizer, binary cross entropy loss and record binary accuracy and recall\n",
    "bert_model.compile(\n",
    "    # Optimizer\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    # Metric used to evaluate model\n",
    "    metrics=[keras.metrics.BinaryAccuracy(), keras.metrics.Recall()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "772d40c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "248/248 [==============================] - 2133s 9s/step - loss: 0.1412 - binary_accuracy: 0.9509\n",
      "Epoch 2/5\n",
      "248/248 [==============================] - 1053s 4s/step - loss: 0.1318 - binary_accuracy: 0.9521\n",
      "Epoch 3/5\n",
      "248/248 [==============================] - 11396s 46s/step - loss: 0.1254 - binary_accuracy: 0.9549\n",
      "Epoch 4/5\n",
      "248/248 [==============================] - 3075s 12s/step - loss: 0.1383 - binary_accuracy: 0.9514\n",
      "Epoch 5/5\n",
      "248/248 [==============================] - 9907s 40s/step - loss: 0.1153 - binary_accuracy: 0.9582\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = bert_model.fit(X_remainder['content'], y_remainder, epochs=5, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa323f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a tensor flow\n",
    "bert_model.save('bert_model_5_relu_sig', save_format='hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322874d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model history to a pickle file\n",
    "joblib.dump(history, 'bert_model_5_relu_sig_hist.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8791a624",
   "metadata": {},
   "source": [
    "Now that the model has been built and saved, it will be loaded and further evaluated in a separate file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone]",
   "language": "python",
   "name": "conda-env-capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
