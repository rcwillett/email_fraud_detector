{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f76708",
   "metadata": {},
   "source": [
    "# Email Fraud Detector: BERT Model Build\n",
    "\n",
    "#### Ross Willett"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5850c",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Email-Fraud-Detector:-BERT-Model-Build\" data-toc-modified-id=\"Email-Fraud-Detector:-BERT-Model-Build-1\">Email Fraud Detector: BERT Model Build</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Ross-Willett\" data-toc-modified-id=\"Ross-Willett-1.0.0.1\">Ross Willett</a></span></li></ul></li></ul></li><li><span><a href=\"#File-Introduction\" data-toc-modified-id=\"File-Introduction-1.1\">File Introduction</a></span></li><li><span><a href=\"#Preparing-the-Model\" data-toc-modified-id=\"Preparing-the-Model-1.2\">Preparing the Model</a></span></li><li><span><a href=\"#Building-the-Model\" data-toc-modified-id=\"Building-the-Model-1.3\">Building the Model</a></span></li><li><span><a href=\"#Other-Builds-with-the-BERT-Model\" data-toc-modified-id=\"Other-Builds-with-the-BERT-Model-1.4\">Other Builds with the BERT Model</a></span></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-1.5\">Conclusions</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33790e",
   "metadata": {},
   "source": [
    "## File Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adedfe1",
   "metadata": {},
   "source": [
    "In this file, a BERT transformer loaded from [TensorflowHub](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4) with additional layers added on top of it for email classification will be trained. A BERT transformer will be used as the base for a transfer learning model for several reasons. One reason a BERT model will be used is due to the fact it uses word embeddings which will place words with similar meanings in a relatively close vector space. This means that the model will not rely on specific words in order to positively identify a fraudulent email, thus making the model more generalizable. In addition to this, the BERT model takes into account the context of words in relation to other words in the input. (Up to 128 words for the small model) This will allow the model to better numerically express the context of the text data which should allow for a better ability to classify an email as fraudulent or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a4e613",
   "metadata": {},
   "source": [
    "## Preparing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860d400d",
   "metadata": {},
   "source": [
    "Before the model can be built and trained, the data needs to be prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "768889ec-9115-4425-b597-48b937c1b9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 12:29:43.229867: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Model selection libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model Evaluation Libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Import Tensor Flow and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Import Tensor Flow Hub and Tensor Flow Text (Required libraries for the pre-trained BERT Model)\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "\n",
    "# Import library for saving files\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b43b1f2-e02c-4997-b455-4d25c47bbfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import warnings and supress them\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9059fde-f475-410c-8ddf-ccca9e618482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Pandas to show all columns / rows\n",
    "pd.options.display.max_columns = 2000\n",
    "pd.options.display.max_rows = 2000\n",
    "# Set column max width larger\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e71215b-9235-44d3-9886-19b84339bc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load X remainder\n",
    "X_remainder = pd.read_csv('./data/X_remainder.csv')\n",
    "# Load X test\n",
    "X_test = pd.read_csv('./data/X_test.csv')\n",
    "# Load y remainder\n",
    "y_remainder = pd.read_csv('./data/y_remainder.csv')\n",
    "# Load y test\n",
    "y_test = pd.read_csv('./data/y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac732b",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68009960",
   "metadata": {},
   "source": [
    "Now that the data has been appropriately separated, the model can be built and trained. First the BERT encoder and pre-trained BERT model needs to be loaded from Tensor Flow Hub.(Encoder located [here](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3), transformer located [here](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4)) The BERT model that will be used is a compact versions of the BERT model so as to allow for faster training and testing. This model consists of 12 hidden layers (i.e. Transformer blocks), a layer node size of 768, and 12 attention heads. For the purposes of this project, the BERT encoding layers will be frozen such that only the layers added to the BERT transformer output will be trained. This will be done since the BERT model has already been trained upon a large data set of words and will output appropriate relationships between these words. Given that the data used to train this model is likely better suited to establish these relationships, these parametric associations will not be adjusted on the basis of the email data. On top of the pooled output of the BERT model, a 128 node layer using RELU activation will be added, and on top of that a 1 node layer with Sigmoid activation. The pooled output of the BERT model will be used since the sequence of words is not important for the purposes of this identification task, thus making the pooled output the most applicable values to pass into other layers. The trainable RELU node layer will be added so as to allow the outputs from the BERT model to be adjusted before passing the results to the Sigmoid layer. The one node Sigmoid layer will be used as it will output a value between 0 and 1 which will be used to classify the email as fraudulent or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b085d065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT encoder from Tensor Flow Hub\n",
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "# Load the BERT model from Tensor Flow Hub\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a22a97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the input layer of the model\n",
    "text_input = layers.Input(shape=(), dtype=tf.string)\n",
    "# Pass the input layer to the BERT tokenizing layer\n",
    "encoder_inputs = preprocessor(text_input)\n",
    "# Pass the tokenized input to the BERT encoder\n",
    "outputs = encoder(encoder_inputs)\n",
    "# Get the pooled output of the BERT encoder\n",
    "pooled_output = outputs[\"pooled_output\"]\n",
    "# Pass the pooled output of the BERT encoder to a 128 node relu layer\n",
    "relu_layer = layers.Dense(128, activation='relu')(pooled_output)\n",
    "# Pass the relu layer to a 1 node sigmoid layer for classification\n",
    "output = layers.Dense(1, activation='sigmoid')(relu_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ced736a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "bert_model = tf.keras.Model(inputs=text_input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de17c82",
   "metadata": {},
   "source": [
    "Now that the model has been built, the various layers associated within this neural network can be examined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d74790b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       {'input_mask': (Non  0           ['input_1[0][0]']                \n",
      "                                e, 128),                                                          \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " keras_layer_1 (KerasLayer)     {'encoder_outputs':  109482241   ['keras_layer[0][0]',            \n",
      "                                 [(None, 128, 768),               'keras_layer[0][1]',            \n",
      "                                 (None, 128, 768),                'keras_layer[0][2]']            \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)],                                               \n",
      "                                 'default': (None,                                                \n",
      "                                768),                                                             \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 768)}                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          98432       ['keras_layer_1[0][13]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,580,802\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Examine the layers of the model\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f6bc0",
   "metadata": {},
   "source": [
    "From the output of the model summary we can see the large number of parameters associated with this model (109,580,802) and the details surrounding each layer of both the BERT encoder and additional layers added on top of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250b0ea",
   "metadata": {},
   "source": [
    "Next the model compiler will be set up to establish how the model will be optimized and how its performance will be evaluated. The model will use the Adam optimizer as this is a fairly standard optimization method used for neural networks. The loss function that will be used is the BinaryCrossentropy loss function since this is the standard loss function used for binary categorization problems in neural networks. Finally the metrics which will be reported for the model are accuracy and recall to establish a baseline of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93380af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the BERT model using the adam optimizer, binary cross entropy loss and record binary accuracy and recall\n",
    "bert_model.compile(\n",
    "    # Optimizer\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    # Metric used to evaluate model\n",
    "    metrics=[keras.metrics.BinaryAccuracy(), keras.metrics.Recall()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5dbf29",
   "metadata": {},
   "source": [
    "Now the model is ready to be trained, it will use a 20% validation split to establish performance and 5 epochs. The reason only 5 epochs will be used in this file is due to the processing power and time required to train this model. Models that are trained with more epochs will be trained on other more powerful machines, but the process is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "772d40c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "248/248 [==============================] - 2133s 9s/step - loss: 0.1412 - binary_accuracy: 0.9509\n",
      "Epoch 2/5\n",
      "248/248 [==============================] - 1053s 4s/step - loss: 0.1318 - binary_accuracy: 0.9521\n",
      "Epoch 3/5\n",
      "248/248 [==============================] - 11396s 46s/step - loss: 0.1254 - binary_accuracy: 0.9549\n",
      "Epoch 4/5\n",
      "248/248 [==============================] - 3075s 12s/step - loss: 0.1383 - binary_accuracy: 0.9514\n",
      "Epoch 5/5\n",
      "248/248 [==============================] - 9907s 40s/step - loss: 0.1153 - binary_accuracy: 0.9582\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = bert_model.fit(X_remainder['content'], y_remainder, epochs=5, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b7f60",
   "metadata": {},
   "source": [
    "The fitted model can now be saved for further use and the history of the fitted model saved for further examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa323f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a tensor flow\n",
    "bert_model.save('bert_model_5_relu_sig', save_format='hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322874d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model history to a pickle file\n",
    "joblib.dump(history.history, 'bert_model_5_relu_sig_hist.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11249945",
   "metadata": {},
   "source": [
    "## Other Builds with the BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f4c052",
   "metadata": {},
   "source": [
    "Using the BERT model as a base, several builds were attempted using Amazon's Sagemaker service. The variations and results for these models will be displayed in the table below. Note that all models built used the BERT uncased preprocessor located [here](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3) and used a single output node with a Sigmoid activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27a587b",
   "metadata": {},
   "source": [
    "| Model Number | BERT Transformer Used            | RELU Activation Layer Nodes | Epochs | Validation Accuracy |\n",
    "|--------------|----------------------------------|-----------------------------|--------|---------------------|\n",
    "| 1            | bert_en_uncased_L-12_H-768_A-12  | 128                         | 5      | 95.5%               |\n",
    "| 2            | bert_en_uncased_L-12_H-768_A-12  | 128                         | 20     | 97.5%               |\n",
    "| 3            | bert_en_uncased_L-12_H-768_A-12  | 256                         | 20     | 95.7%               |\n",
    "| 4            | bert_en_uncased_L-12_H-768_A-12  | 0                           | 20     | 95.2%               |\n",
    "| 5            | bert_en_uncased_L-24_H-1024_A-16 | 128                         | 20     | 89.4%               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057d21c",
   "metadata": {},
   "source": [
    "As demonstrated by the table above, the model which resulted in the highest validation accuracy utilized the \"bert_en_uncased_L-12_H-768_A-12\" tensor flow hub module with a 128 Relu activation layer and ran for 20 epochs. This will be the model used for final evaluation and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a34033c",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874ef5e",
   "metadata": {},
   "source": [
    "Using transfer learning with a BERT transformer model should improve general applicability and performance of fraud email identification. This model will utilize a small BERT transformer from tensor flow hub as a base with a Relu activation layer using 128 nodes on the output from it and a Sigmoid activation layer with 1 node for output. This model will undergo further testing and evaluation in subsequent files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone]",
   "language": "python",
   "name": "conda-env-capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
